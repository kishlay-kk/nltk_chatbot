{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kishlay-kk/nltk_chatbot/blob/master/Tokenising_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "OLNahZcm8e_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Tokenisation and Tagging of Text**\n",
        "In order to classify and analyze a body of text in a more granular fashion, it is necessary to consider how to break it into individual sentences and words or \"tokens\". Broadly then there are two tasks:\n",
        "\n",
        "***Sentence Tokenization***\n",
        "***Word Tokenization*** \n",
        "To go beyond counting the frequency or occurence of actual words we need to classify words in general categories that signify their part in the construct of the sentence - for instance Noun, Verb Adjective etc. This is generally known as\n",
        "\n",
        "Part of Speech or POS Tagging"
      ]
    },
    {
      "metadata": {
        "id": "RR2CHVFs84nB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Sentence Tokenisation***\n"
      ]
    },
    {
      "metadata": {
        "id": "7SKs8TOr9Bui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "32e56b87-a667-4fb4-acd0-0a9d2fd3e755"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "ulysses = \"Mrkgnao! the cat said loudly. She blinked up out of her avid shameclosing eyes, mewing \\\n",
        "plaintively and long, showing him her milkwhite teeth. He watched the dark eyeslits narrowing \\\n",
        "with greed till her eyes were green stones. Then he went to the dresser, took the jug Hanlon's\\\n",
        "milkman had just filled for him, poured warmbubbled milk on a saucer and set it slowly on the floor.\\\n",
        "— Gurrhr! she cried, running to lap.\"\n",
        "\n",
        "doc = nltk.sent_tokenize(ulysses)     #sent_tokenize:- Sentence Tokenize\n",
        "for s in doc:\n",
        "    print(\">\",s)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "> Mrkgnao!\n",
            "> the cat said loudly.\n",
            "> She blinked up out of her avid shameclosing eyes, mewing plaintively and long, showing him her milkwhite teeth.\n",
            "> He watched the dark eyeslits narrowing with greed till her eyes were green stones.\n",
            "> Then he went to the dresser, took the jug Hanlon'smilkman had just filled for him, poured warmbubbled milk on a saucer and set it slowly on the floor.— Gurrhr!\n",
            "> she cried, running to lap.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lTkbLWrT-7rB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Word Tokenisation***\n",
        "\n",
        "There are many methods for tokenising text into words. The default Penn Treebank Tokeniser is the tokeniser based on the Penn TreeBank Corpus. A few examples of different tokenisers giving different results are listed below:\n",
        "\n",
        "TreebankWordTokenizer\n",
        "WordPunctTokenizer\n",
        "WhitespaceTokenize\n",
        "\n",
        "We can see a simple illustration of the impact of chosing a different tokenisation method by looking at the different results we get for a simple sentence:"
      ]
    },
    {
      "metadata": {
        "id": "8BblCbjq_Rrb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c9c4e31e-cdac-4b28-e98e-7fa07c841885"
      },
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "sentence = \"Mary had a little lamb it's fleece was white as snow.\"\n",
        "\n",
        "#Default Tokenisers (Tree Tookeniser)\n",
        "tree_tokens=word_tokenize(sentence)\n",
        "print(\"DEFAULT: \",tree_tokens)\n",
        "\n",
        "                                 # Other Tokenisers\n",
        "#Tokenising on the basis of punctuations\n",
        "punct_tokenizer = nltk.tokenize.WordPunctTokenizer()                                 #punct_tokenizer is a object of the class WordPunctTokenizer\n",
        "punct_tokens = punct_tokenizer.tokenize(sentence)                                    # .tokenize is the function\n",
        "print(\"PUNCTUATION : \",punct_tokens)\n",
        "\n",
        "#Tokenising on the basis of spaces\n",
        "space_tokenizer = nltk.tokenize.SpaceTokenizer()\n",
        "space_tokens = space_tokenizer.tokenize(sentence)\n",
        "print(\"SPACES : \",space_tokens)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DEFAULT:  ['Mary', 'had', 'a', 'little', 'lamb', 'it', \"'s\", 'fleece', 'was', 'white', 'as', 'snow', '.']\n",
            "PUNCTUATION :  ['Mary', 'had', 'a', 'little', 'lamb', 'it', \"'\", 's', 'fleece', 'was', 'white', 'as', 'snow', '.']\n",
            "SPACES :  ['Mary', 'had', 'a', 'little', 'lamb', \"it's\", 'fleece', 'was', 'white', 'as', 'snow.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_JlBfv0oCrV-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Part of Speech Tagging***\n",
        "\n",
        "For each word-token the nltk pos_tag method can be used to classify its Part of Speech (POS), automating the classification of words into their parts of speech and labeling them accordingly.\n",
        "\n",
        "The outcome depends on how the sentence has been split up into individual tokens and which Tokensizer and Corpus the POS-tagger has been trained against:"
      ]
    },
    {
      "metadata": {
        "id": "CvDI6kVbCgPk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f530a39a-35d9-4597-bda3-bea05aab3450"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')           #Library File required to do tagging\n",
        "pos = nltk.pos_tag(tree_tokens)\n",
        "print(pos)\n",
        "pos_space = nltk.pos_tag(space_tokens)\n",
        "print(pos_space)\n",
        "pos_space = nltk.pos_tag(punct_tokens)\n",
        "print(pos_space)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('Mary', 'NNP'), ('had', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('lamb', 'NN'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('fleece', 'NN'), ('was', 'VBD'), ('white', 'JJ'), ('as', 'IN'), ('snow', 'NN'), ('.', '.')]\n",
            "[('Mary', 'NNP'), ('had', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('lamb', 'JJ'), (\"it's\", 'NN'), ('fleece', 'NN'), ('was', 'VBD'), ('white', 'JJ'), ('as', 'IN'), ('snow.', 'NN')]\n",
            "[('Mary', 'NNP'), ('had', 'VBD'), ('a', 'DT'), ('little', 'JJ'), ('lamb', 'NN'), ('it', 'PRP'), (\"'\", \"''\"), ('s', 'JJ'), ('fleece', 'NN'), ('was', 'VBD'), ('white', 'JJ'), ('as', 'IN'), ('snow', 'NN'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-IZUHEaJDbYP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "ntscI549kubr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These are the tags under which words can be classified by the POS Tagger\n",
        "\n",
        "#### PoS Tag Descriptions ###\n",
        "CC | Coordinating conjunction  \n",
        "CD | Cardinal number  \n",
        "DT | Determiner  \n",
        "EX | Existential there  \n",
        "FW | Foreign word  \n",
        "IN | Preposition or subordinating conjunction  \n",
        "JJ | Adjective  \n",
        "JJR | Adjective, comparative  \n",
        "JJS | Adjective, superlative  \n",
        "LS | List item marker  \n",
        "MD | Modal  \n",
        "NN | Noun, singular or mass  \n",
        "NNS | Noun, plural  \n",
        "NNP | Proper noun, singular  \n",
        "NNPS | Proper noun, plural  \n",
        "PDT | Predeterminer  \n",
        "POS | Possessive ending  \n",
        "PRP | Personal pronoun  \n",
        "PRP\\$ | Possessive pronoun  \n",
        "RB | Adverb  \n",
        "RBR | Adverb, comparative  \n",
        "RBS | Adverb, superlative  \n",
        "RP | Particle  \n",
        "SYM | Symbol  \n",
        "TO | to  \n",
        "UH | Interjection  \n",
        "VB | Verb, base form  \n",
        "VBD | Verb, past tense  \n",
        "VBG | Verb, gerund or present participle  \n",
        "VBN | Verb, past participle  \n",
        "VBP | Verb, non-3rd person singular present  \n",
        "VBZ | Verb, 3rd person singular present  \n",
        "WDT | Wh-determiner  \n",
        "WP | Wh-pronoun  \n",
        "WP$ | Possessive wh-pronoun  \n",
        "WRB | Wh-adverb   \n"
      ]
    },
    {
      "metadata": {
        "id": "2NJQ6LNkDvag",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This Categorization can be used to derrive or obtain more specefic data from a text like  obtaining only nouns or pronouns etc"
      ]
    },
    {
      "metadata": {
        "id": "d1VEb0mFEPdi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cd49128c-04ac-4db4-c3d4-6b0b8c57654d"
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "regex = re.compile(\"^N.*\")                                        #means categories starting from N// .* means rest can be anything \n",
        "nouns = []\n",
        "for l in pos:                                                     #pos is the collection of tree tokenised data\n",
        "    if regex.match(l[1]):\n",
        "        nouns.append(l[0])\n",
        "print(\"Nouns:\", nouns)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nouns: ['Mary', 'lamb', 'fleece', 'snow']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "96Two8dXFId8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "***Stemming and Lemmatizing***\n",
        "\n",
        "Striping off the suffixes from words is known as stemming.\n",
        "Mapping a word to a known dictionary word is know as lemmatization\n",
        "\n",
        "There are multiple Stemming methods available and the the NLTK book references a few methods in particular:\n",
        "\n",
        "The Porter Stemmer - see https://tartarus.org/martin/PorterStemmer/\n",
        "Lancaster Stemmer - (Chris Paice, University of Lancaster) additionally the\n",
        "Snowball Stemmer - \"Porter 2\" developed by Martin Porter is generally considered the de-facto optimal Stemmer\n",
        "A list of other stemming methods can be found here: http://www.nltk.org/api/nltk.stem.html. Current Stemming and \"Lemming\" techniques are an inexact process as things currently stand.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "These dont have a much significant practical application"
      ]
    },
    {
      "metadata": {
        "id": "oYTL1w-bEbMl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}